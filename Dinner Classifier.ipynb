{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import rec_parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Data\n",
    "To obtain data, a recipe is parsed to identify dictionary matches. Each dictionary entry corresponds to an index in recipe space. So, each recipe is represented as a vector in recipe space of dimension 4060. Beginnning with a set of 15768 recipe vectors. 10% of those vectors, at equal intervals are reserved as test data. The result should be 4 matrices, trainX representing 14191 recipe vectors, trainY representing binary labels for each of those recipes, testX representing 1577 recipe vectors on which to test, and testY reresenting labels for the test data. For the label matrices, a 1 in column 0 represents 'dinner', while a 1 in column 1 represents 'non-dinner'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your data is ready.\n",
      "trainX shape = (14190, 4060)\n",
      "trainY shape = (14190, 2)\n",
      "testX shape = (1577, 4060)\n",
      "testY shape = (1577, 2)\n"
     ]
    }
   ],
   "source": [
    "data = rec_parse.parse_recipes()\n",
    "trainX, testX = rec_parse.split(data)\n",
    "\n",
    "trainY = np.zeros((len(data)-1577, 2), dtype=np.int)\n",
    "testY = np.zeros([1577, 2], dtype=np.int)\n",
    "\n",
    "for ind, row in enumerate(trainY):\n",
    "    if ind > 1500:\n",
    "        trainY[ind][1] = 1\n",
    "    else:\n",
    "        trainY[ind][0] = 1\n",
    "\n",
    "for ind, row in enumerate(testY):\n",
    "    if ind > 150:\n",
    "        testY[ind][1] = 1\n",
    "    else:\n",
    "        testY[ind][0] = 1\n",
    "        \n",
    "print(\"Your data is ready.\")\n",
    "print(\"trainX shape =\", trainX.shape)\n",
    "print(\"trainY shape =\", trainY.shape)\n",
    "print(\"testX shape =\", testX.shape)\n",
    "print(\"testY shape =\", testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize graph features\n",
    "\n",
    "At line 4, numEpochs is set very low at 100. This low number of epochs results in a model that builds quickly, but has low accuracy. to experiment with a more accurate model, increase numEpochs to at least 2000. To train a production model, you would likely run close to 30k, and probably continue running until cost no longer changes, or does so minimally from epoch to epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numFeatures = trainX.shape[1]\n",
    "numLabels = trainY.shape[1]\n",
    "\n",
    "numEpochs = 1000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learningRate = tf.train.exponential_decay(learning_rate=0.0008,\n",
    "                                          global_step= 1,\n",
    "                                          decay_steps=trainX.shape[0],\n",
    "                                          decay_rate= 0.95,\n",
    "                                          staircase=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, numFeatures])\n",
    "yGold = tf.placeholder(tf.float32, [None, numLabels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Variable/read:0\", shape=(4060, 2), dtype=float32)\n",
      "Tensor(\"Variable_1/read:0\", shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "weights = tf.Variable(tf.random_normal([numFeatures,numLabels],\n",
    "                                       mean=0,\n",
    "                                       stddev=(np.sqrt(6/numFeatures+\n",
    "                                                         numLabels+1)),\n",
    "                                       name=\"weights\"))\n",
    "\n",
    "bias = tf.Variable(tf.random_normal([1,numLabels],\n",
    "                                    mean=0,\n",
    "                                    stddev=(np.sqrt(6/numFeatures+numLabels+1)),\n",
    "                                    name=\"bias\"))\n",
    "\n",
    "init_OP = tf.global_variables_initializer()\n",
    "\n",
    "for i in tf.global_variables():\n",
    "    print(i)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The algorithm\n",
    "Multiply trainX(14191, 4060) by weights(4060, 2), resulting in apply_weights_OP(14191, 2).\n",
    "And add the bias(1, 2), resulting in add_bias_OP(14191, 2)\n",
    "\n",
    "Activate using rectified linear:\n",
    "f(x) = max(0, x)\n",
    "\n",
    "Calculate loss using Mean Squared Error:\n",
    "$\\frac{1}{n}\\sum_{i=0}^n ({y}_{estimated}-{y}_{real})^2$\n",
    "\n",
    "Optimize using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FEEDFORWARD ALGORITHM\n",
    "apply_weights_OP = tf.matmul(X, weights, name=\"apply_weights\")\n",
    "add_bias_OP = tf.add(apply_weights_OP, bias, name=\"add_bias\")\n",
    "activation_OP = tf.nn.sigmoid(add_bias_OP, name=\"activation\")\n",
    "\n",
    "# COST FUNCTION i.e. MEAN SQUARED ERROR\n",
    "cost_OP = tf.nn.l2_loss(activation_OP-yGold, name=\"squared_error_cost\")\n",
    "\n",
    "# OPTIMIZATION ALGORITHM i.e. GRADIENT DESCENT\n",
    "training_OP = tf.train.GradientDescentOptimizer(learningRate).minimize(cost_OP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.609725\n",
      "step 0, cost 5571.9\n",
      "step 0, change in cost 5571.9\n",
      "step 10, training accuracy 0.872657\n",
      "step 10, cost 2054.25\n",
      "step 10, change in cost 3517.65\n",
      "step 20, training accuracy 0.882734\n",
      "step 20, cost 1814.12\n",
      "step 20, change in cost 240.139\n",
      "step 30, training accuracy 0.885483\n",
      "step 30, cost 1723.1\n",
      "step 30, change in cost 91.0184\n",
      "step 40, training accuracy 0.886892\n",
      "step 40, cost 1678.89\n",
      "step 40, change in cost 44.2075\n",
      "step 50, training accuracy 0.888583\n",
      "step 50, cost 1651.03\n",
      "step 50, change in cost 27.8577\n",
      "step 60, training accuracy 0.889641\n",
      "step 60, cost 1630.09\n",
      "step 60, change in cost 20.9387\n",
      "step 70, training accuracy 0.890345\n",
      "step 70, cost 1614.35\n",
      "step 70, change in cost 15.7411\n",
      "step 80, training accuracy 0.89105\n",
      "step 80, cost 1600.67\n",
      "step 80, change in cost 13.6858\n",
      "step 90, training accuracy 0.891332\n",
      "step 90, cost 1589.38\n",
      "step 90, change in cost 11.2836\n",
      "step 100, training accuracy 0.891543\n",
      "step 100, cost 1580.85\n",
      "step 100, change in cost 8.53333\n",
      "step 110, training accuracy 0.891966\n",
      "step 110, cost 1574.38\n",
      "step 110, change in cost 6.47302\n",
      "step 120, training accuracy 0.892107\n",
      "step 120, cost 1569.11\n",
      "step 120, change in cost 5.26575\n",
      "step 130, training accuracy 0.892389\n",
      "step 130, cost 1564.99\n",
      "step 130, change in cost 4.12366\n",
      "step 140, training accuracy 0.89253\n",
      "step 140, cost 1561.46\n",
      "step 140, change in cost 3.52356\n",
      "step 150, training accuracy 0.892812\n",
      "step 150, cost 1558.35\n",
      "step 150, change in cost 3.11035\n",
      "step 160, training accuracy 0.892882\n",
      "step 160, cost 1555.61\n",
      "step 160, change in cost 2.74756\n",
      "step 170, training accuracy 0.892953\n",
      "step 170, cost 1553.11\n",
      "step 170, change in cost 2.4978\n",
      "step 180, training accuracy 0.892953\n",
      "step 180, cost 1550.75\n",
      "step 180, change in cost 2.35706\n",
      "step 190, training accuracy 0.893023\n",
      "step 190, cost 1548.54\n",
      "step 190, change in cost 2.21582\n",
      "step 200, training accuracy 0.893094\n",
      "step 200, cost 1546.52\n",
      "step 200, change in cost 2.01746\n",
      "step 210, training accuracy 0.893164\n",
      "step 210, cost 1544.71\n",
      "step 210, change in cost 1.80762\n",
      "step 220, training accuracy 0.893305\n",
      "step 220, cost 1543.07\n",
      "step 220, change in cost 1.64233\n",
      "step 230, training accuracy 0.893305\n",
      "step 230, cost 1541.55\n",
      "step 230, change in cost 1.51917\n",
      "step 240, training accuracy 0.893376\n",
      "step 240, cost 1540.15\n",
      "step 240, change in cost 1.39844\n",
      "step 250, training accuracy 0.893376\n",
      "step 250, cost 1538.88\n",
      "step 250, change in cost 1.26709\n",
      "step 260, training accuracy 0.893376\n",
      "step 260, cost 1537.73\n",
      "step 260, change in cost 1.14954\n",
      "step 270, training accuracy 0.893376\n",
      "step 270, cost 1536.67\n",
      "step 270, change in cost 1.06201\n",
      "step 280, training accuracy 0.893376\n",
      "step 280, cost 1535.67\n",
      "step 280, change in cost 0.998779\n",
      "step 290, training accuracy 0.893446\n",
      "step 290, cost 1534.72\n",
      "step 290, change in cost 0.952759\n",
      "step 300, training accuracy 0.893517\n",
      "step 300, cost 1533.81\n",
      "step 300, change in cost 0.91394\n",
      "step 310, training accuracy 0.893517\n",
      "step 310, cost 1532.93\n",
      "step 310, change in cost 0.876343\n",
      "step 320, training accuracy 0.893517\n",
      "step 320, cost 1532.09\n",
      "step 320, change in cost 0.835205\n",
      "step 330, training accuracy 0.893517\n",
      "step 330, cost 1531.3\n",
      "step 330, change in cost 0.793579\n",
      "step 340, training accuracy 0.893517\n",
      "step 340, cost 1530.55\n",
      "step 340, change in cost 0.753174\n",
      "step 350, training accuracy 0.893517\n",
      "step 350, cost 1529.83\n",
      "step 350, change in cost 0.718262\n",
      "step 360, training accuracy 0.893517\n",
      "step 360, cost 1529.14\n",
      "step 360, change in cost 0.688477\n",
      "step 370, training accuracy 0.893446\n",
      "step 370, cost 1528.48\n",
      "step 370, change in cost 0.665771\n",
      "step 380, training accuracy 0.893446\n",
      "step 380, cost 1527.83\n",
      "step 380, change in cost 0.646362\n",
      "step 390, training accuracy 0.893446\n",
      "step 390, cost 1527.2\n",
      "step 390, change in cost 0.626831\n",
      "step 400, training accuracy 0.893587\n",
      "step 400, cost 1526.6\n",
      "step 400, change in cost 0.603027\n",
      "step 410, training accuracy 0.893587\n",
      "step 410, cost 1526.03\n",
      "step 410, change in cost 0.572754\n",
      "step 420, training accuracy 0.893587\n",
      "step 420, cost 1525.49\n",
      "step 420, change in cost 0.540039\n",
      "step 430, training accuracy 0.893587\n",
      "step 430, cost 1524.98\n",
      "step 430, change in cost 0.50769\n",
      "step 440, training accuracy 0.893587\n",
      "step 440, cost 1524.5\n",
      "step 440, change in cost 0.476685\n",
      "step 450, training accuracy 0.893587\n",
      "step 450, cost 1524.05\n",
      "step 450, change in cost 0.448242\n",
      "step 460, training accuracy 0.893587\n",
      "step 460, cost 1523.63\n",
      "step 460, change in cost 0.420166\n",
      "step 470, training accuracy 0.893587\n",
      "step 470, cost 1523.24\n",
      "step 470, change in cost 0.393311\n",
      "step 480, training accuracy 0.893587\n",
      "step 480, cost 1522.87\n",
      "step 480, change in cost 0.368652\n",
      "step 490, training accuracy 0.893587\n",
      "step 490, cost 1522.52\n",
      "step 490, change in cost 0.34668\n",
      "step 500, training accuracy 0.893587\n",
      "step 500, cost 1522.2\n",
      "step 500, change in cost 0.327637\n",
      "step 510, training accuracy 0.893587\n",
      "step 510, cost 1521.89\n",
      "step 510, change in cost 0.311768\n",
      "step 520, training accuracy 0.893587\n",
      "step 520, cost 1521.59\n",
      "step 520, change in cost 0.298584\n",
      "step 530, training accuracy 0.893587\n",
      "step 530, cost 1521.3\n",
      "step 530, change in cost 0.287476\n",
      "step 540, training accuracy 0.893587\n",
      "step 540, cost 1521.02\n",
      "step 540, change in cost 0.279907\n",
      "step 550, training accuracy 0.893587\n",
      "step 550, cost 1520.75\n",
      "step 550, change in cost 0.273315\n",
      "step 560, training accuracy 0.893587\n",
      "step 560, cost 1520.48\n",
      "step 560, change in cost 0.270386\n",
      "step 570, training accuracy 0.893587\n",
      "step 570, cost 1520.21\n",
      "step 570, change in cost 0.268555\n",
      "step 580, training accuracy 0.893587\n",
      "step 580, cost 1519.94\n",
      "step 580, change in cost 0.269531\n",
      "step 590, training accuracy 0.893587\n",
      "step 590, cost 1519.67\n",
      "step 590, change in cost 0.271851\n",
      "step 600, training accuracy 0.893587\n",
      "step 600, cost 1519.39\n",
      "step 600, change in cost 0.275146\n",
      "step 610, training accuracy 0.893587\n",
      "step 610, cost 1519.11\n",
      "step 610, change in cost 0.279907\n",
      "step 620, training accuracy 0.893587\n",
      "step 620, cost 1518.83\n",
      "step 620, change in cost 0.28418\n",
      "step 630, training accuracy 0.893587\n",
      "step 630, cost 1518.54\n",
      "step 630, change in cost 0.286743\n",
      "step 640, training accuracy 0.893587\n",
      "step 640, cost 1518.25\n",
      "step 640, change in cost 0.286743\n",
      "step 650, training accuracy 0.893587\n",
      "step 650, cost 1517.97\n",
      "step 650, change in cost 0.283447\n",
      "step 660, training accuracy 0.893587\n",
      "step 660, cost 1517.69\n",
      "step 660, change in cost 0.276611\n",
      "step 670, training accuracy 0.893658\n",
      "step 670, cost 1517.43\n",
      "step 670, change in cost 0.266968\n",
      "step 680, training accuracy 0.893658\n",
      "step 680, cost 1517.17\n",
      "step 680, change in cost 0.256348\n",
      "step 690, training accuracy 0.893658\n",
      "step 690, cost 1516.92\n",
      "step 690, change in cost 0.244995\n",
      "step 700, training accuracy 0.893658\n",
      "step 700, cost 1516.69\n",
      "step 700, change in cost 0.234375\n",
      "step 710, training accuracy 0.893658\n",
      "step 710, cost 1516.47\n",
      "step 710, change in cost 0.224731\n",
      "step 720, training accuracy 0.893658\n",
      "step 720, cost 1516.25\n",
      "step 720, change in cost 0.21521\n",
      "step 730, training accuracy 0.893658\n",
      "step 730, cost 1516.04\n",
      "step 730, change in cost 0.207031\n",
      "step 740, training accuracy 0.893658\n",
      "step 740, cost 1515.84\n",
      "step 740, change in cost 0.200439\n",
      "step 750, training accuracy 0.893658\n",
      "step 750, cost 1515.65\n",
      "step 750, change in cost 0.193848\n",
      "step 760, training accuracy 0.893658\n",
      "step 760, cost 1515.46\n",
      "step 760, change in cost 0.188721\n",
      "step 770, training accuracy 0.893658\n",
      "step 770, cost 1515.28\n",
      "step 770, change in cost 0.183716\n",
      "step 780, training accuracy 0.893658\n",
      "step 780, cost 1515.1\n",
      "step 780, change in cost 0.180054\n",
      "step 790, training accuracy 0.893658\n",
      "step 790, cost 1514.92\n",
      "step 790, change in cost 0.175903\n",
      "step 800, training accuracy 0.893658\n",
      "step 800, cost 1514.75\n",
      "step 800, change in cost 0.171997\n",
      "step 810, training accuracy 0.893658\n",
      "step 810, cost 1514.58\n",
      "step 810, change in cost 0.167969\n",
      "step 820, training accuracy 0.893658\n",
      "step 820, cost 1514.42\n",
      "step 820, change in cost 0.163696\n",
      "step 830, training accuracy 0.893658\n",
      "step 830, cost 1514.26\n",
      "step 830, change in cost 0.15918\n",
      "step 840, training accuracy 0.893658\n",
      "step 840, cost 1514.1\n",
      "step 840, change in cost 0.154297\n",
      "step 850, training accuracy 0.893658\n",
      "step 850, cost 1513.95\n",
      "step 850, change in cost 0.149536\n",
      "step 860, training accuracy 0.893658\n",
      "step 860, cost 1513.81\n",
      "step 860, change in cost 0.14502\n",
      "step 870, training accuracy 0.893658\n",
      "step 870, cost 1513.67\n",
      "step 870, change in cost 0.140991\n",
      "step 880, training accuracy 0.893658\n",
      "step 880, cost 1513.53\n",
      "step 880, change in cost 0.137329\n",
      "step 890, training accuracy 0.893658\n",
      "step 890, cost 1513.4\n",
      "step 890, change in cost 0.134155\n",
      "step 900, training accuracy 0.893658\n",
      "step 900, cost 1513.27\n",
      "step 900, change in cost 0.131348\n",
      "step 910, training accuracy 0.893658\n",
      "step 910, cost 1513.14\n",
      "step 910, change in cost 0.128906\n",
      "step 920, training accuracy 0.893658\n",
      "step 920, cost 1513.01\n",
      "step 920, change in cost 0.126831\n",
      "step 930, training accuracy 0.893658\n",
      "step 930, cost 1512.89\n",
      "step 930, change in cost 0.124023\n",
      "step 940, training accuracy 0.893658\n",
      "step 940, cost 1512.76\n",
      "step 940, change in cost 0.121338\n",
      "step 950, training accuracy 0.893658\n",
      "step 950, cost 1512.65\n",
      "step 950, change in cost 0.11792\n",
      "step 960, training accuracy 0.893658\n",
      "step 960, cost 1512.53\n",
      "step 960, change in cost 0.114502\n",
      "step 970, training accuracy 0.893658\n",
      "step 970, cost 1512.42\n",
      "step 970, change in cost 0.110962\n",
      "step 980, training accuracy 0.893658\n",
      "step 980, cost 1512.31\n",
      "step 980, change in cost 0.107056\n",
      "step 990, training accuracy 0.893658\n",
      "step 990, cost 1512.21\n",
      "step 990, change in cost 0.10376\n",
      "step 1000, training accuracy 0.893658\n",
      "step 1000, cost 1512.11\n",
      "step 1000, change in cost 0.100464\n",
      "step 1010, training accuracy 0.893658\n",
      "step 1010, cost 1512.01\n",
      "step 1010, change in cost 0.0980225\n",
      "step 1020, training accuracy 0.893658\n",
      "step 1020, cost 1511.92\n",
      "step 1020, change in cost 0.095459\n",
      "step 1030, training accuracy 0.893658\n",
      "step 1030, cost 1511.82\n",
      "step 1030, change in cost 0.09375\n",
      "step 1040, training accuracy 0.893658\n",
      "step 1040, cost 1511.73\n",
      "step 1040, change in cost 0.0915527\n",
      "step 1050, training accuracy 0.893658\n",
      "step 1050, cost 1511.64\n",
      "step 1050, change in cost 0.0909424\n",
      "step 1060, training accuracy 0.893658\n",
      "step 1060, cost 1511.55\n",
      "step 1060, change in cost 0.0899658\n",
      "step 1070, training accuracy 0.893658\n",
      "step 1070, cost 1511.46\n",
      "step 1070, change in cost 0.0894775\n",
      "step 1080, training accuracy 0.893658\n",
      "step 1080, cost 1511.37\n",
      "step 1080, change in cost 0.0888672\n",
      "step 1090, training accuracy 0.893658\n",
      "step 1090, cost 1511.28\n",
      "step 1090, change in cost 0.0888672\n",
      "step 1100, training accuracy 0.893658\n",
      "step 1100, cost 1511.19\n",
      "step 1100, change in cost 0.0888672\n",
      "step 1110, training accuracy 0.893658\n",
      "step 1110, cost 1511.1\n",
      "step 1110, change in cost 0.0887451\n",
      "step 1120, training accuracy 0.893728\n",
      "step 1120, cost 1511.02\n",
      "step 1120, change in cost 0.0895996\n",
      "step 1130, training accuracy 0.893728\n",
      "step 1130, cost 1510.93\n",
      "step 1130, change in cost 0.0900879\n",
      "step 1140, training accuracy 0.893728\n",
      "step 1140, cost 1510.83\n",
      "step 1140, change in cost 0.090332\n",
      "step 1150, training accuracy 0.893728\n",
      "step 1150, cost 1510.74\n",
      "step 1150, change in cost 0.0915527\n",
      "step 1160, training accuracy 0.893728\n",
      "step 1160, cost 1510.65\n",
      "step 1160, change in cost 0.0922852\n",
      "step 1170, training accuracy 0.893728\n",
      "step 1170, cost 1510.56\n",
      "step 1170, change in cost 0.0931396\n",
      "step 1180, training accuracy 0.893728\n",
      "step 1180, cost 1510.46\n",
      "step 1180, change in cost 0.0935059\n",
      "step 1190, training accuracy 0.893728\n",
      "step 1190, cost 1510.37\n",
      "step 1190, change in cost 0.0948486\n",
      "step 1200, training accuracy 0.893728\n",
      "step 1200, cost 1510.27\n",
      "step 1200, change in cost 0.0952148\n",
      "step 1210, training accuracy 0.893728\n",
      "step 1210, cost 1510.18\n",
      "step 1210, change in cost 0.0957031\n",
      "step 1220, training accuracy 0.893728\n",
      "step 1220, cost 1510.08\n",
      "step 1220, change in cost 0.0961914\n",
      "step 1230, training accuracy 0.893728\n",
      "step 1230, cost 1509.99\n",
      "step 1230, change in cost 0.0966797\n",
      "step 1240, training accuracy 0.893728\n",
      "step 1240, cost 1509.89\n",
      "step 1240, change in cost 0.0961914\n",
      "step 1250, training accuracy 0.893728\n",
      "step 1250, cost 1509.79\n",
      "step 1250, change in cost 0.0965576\n",
      "step 1260, training accuracy 0.893728\n",
      "step 1260, cost 1509.7\n",
      "step 1260, change in cost 0.0965576\n",
      "step 1270, training accuracy 0.893728\n",
      "step 1270, cost 1509.6\n",
      "step 1270, change in cost 0.0958252\n",
      "step 1280, training accuracy 0.893728\n",
      "step 1280, cost 1509.5\n",
      "step 1280, change in cost 0.0963135\n",
      "step 1290, training accuracy 0.893728\n",
      "step 1290, cost 1509.41\n",
      "step 1290, change in cost 0.0958252\n",
      "step 1300, training accuracy 0.893728\n",
      "step 1300, cost 1509.31\n",
      "step 1300, change in cost 0.0953369\n",
      "step 1310, training accuracy 0.893728\n",
      "step 1310, cost 1509.22\n",
      "step 1310, change in cost 0.0952148\n",
      "step 1320, training accuracy 0.893728\n",
      "step 1320, cost 1509.12\n",
      "step 1320, change in cost 0.0944824\n",
      "step 1330, training accuracy 0.893728\n",
      "step 1330, cost 1509.03\n",
      "step 1330, change in cost 0.09375\n",
      "step 1340, training accuracy 0.893728\n",
      "step 1340, cost 1508.94\n",
      "step 1340, change in cost 0.0935059\n",
      "step 1350, training accuracy 0.893728\n",
      "step 1350, cost 1508.84\n",
      "step 1350, change in cost 0.0925293\n",
      "step 1360, training accuracy 0.893728\n",
      "step 1360, cost 1508.75\n",
      "step 1360, change in cost 0.0916748\n",
      "step 1370, training accuracy 0.893728\n",
      "step 1370, cost 1508.66\n",
      "step 1370, change in cost 0.0904541\n",
      "step 1380, training accuracy 0.893728\n",
      "step 1380, cost 1508.57\n",
      "step 1380, change in cost 0.0900879\n",
      "step 1390, training accuracy 0.893728\n",
      "step 1390, cost 1508.48\n",
      "step 1390, change in cost 0.0888672\n",
      "step 1400, training accuracy 0.893728\n",
      "step 1400, cost 1508.4\n",
      "step 1400, change in cost 0.0874023\n",
      "step 1410, training accuracy 0.893728\n",
      "step 1410, cost 1508.31\n",
      "step 1410, change in cost 0.0863037\n",
      "step 1420, training accuracy 0.893728\n",
      "step 1420, cost 1508.22\n",
      "step 1420, change in cost 0.0847168\n",
      "step 1430, training accuracy 0.893728\n",
      "step 1430, cost 1508.14\n",
      "step 1430, change in cost 0.0836182\n",
      "step 1440, training accuracy 0.893798\n",
      "step 1440, cost 1508.06\n",
      "step 1440, change in cost 0.0819092\n",
      "step 1450, training accuracy 0.893798\n",
      "step 1450, cost 1507.98\n",
      "step 1450, change in cost 0.0804443\n",
      "step 1460, training accuracy 0.893798\n",
      "step 1460, cost 1507.9\n",
      "step 1460, change in cost 0.0789795\n",
      "step 1470, training accuracy 0.893798\n",
      "step 1470, cost 1507.82\n",
      "step 1470, change in cost 0.0770264\n",
      "step 1480, training accuracy 0.893798\n",
      "step 1480, cost 1507.75\n",
      "step 1480, change in cost 0.0758057\n",
      "step 1490, training accuracy 0.893798\n",
      "step 1490, cost 1507.67\n",
      "step 1490, change in cost 0.0739746\n",
      "step 1500, training accuracy 0.893798\n",
      "step 1500, cost 1507.6\n",
      "step 1500, change in cost 0.072876\n",
      "step 1510, training accuracy 0.893798\n",
      "step 1510, cost 1507.53\n",
      "step 1510, change in cost 0.071167\n",
      "step 1520, training accuracy 0.893798\n",
      "step 1520, cost 1507.46\n",
      "step 1520, change in cost 0.0697021\n",
      "step 1530, training accuracy 0.893798\n",
      "step 1530, cost 1507.39\n",
      "step 1530, change in cost 0.0688477\n",
      "step 1540, training accuracy 0.893869\n",
      "step 1540, cost 1507.32\n",
      "step 1540, change in cost 0.067627\n",
      "step 1550, training accuracy 0.893869\n",
      "step 1550, cost 1507.26\n",
      "step 1550, change in cost 0.0661621\n",
      "step 1560, training accuracy 0.893869\n",
      "step 1560, cost 1507.19\n",
      "step 1560, change in cost 0.0654297\n",
      "step 1570, training accuracy 0.893869\n",
      "step 1570, cost 1507.13\n",
      "step 1570, change in cost 0.0644531\n",
      "step 1580, training accuracy 0.893869\n",
      "step 1580, cost 1507.06\n",
      "step 1580, change in cost 0.0634766\n",
      "step 1590, training accuracy 0.893869\n",
      "step 1590, cost 1507\n",
      "step 1590, change in cost 0.0625\n",
      "step 1600, training accuracy 0.893869\n",
      "step 1600, cost 1506.94\n",
      "step 1600, change in cost 0.0618896\n",
      "step 1610, training accuracy 0.893869\n",
      "step 1610, cost 1506.88\n",
      "step 1610, change in cost 0.0611572\n",
      "step 1620, training accuracy 0.893869\n",
      "step 1620, cost 1506.82\n",
      "step 1620, change in cost 0.0600586\n",
      "step 1630, training accuracy 0.893869\n",
      "step 1630, cost 1506.76\n",
      "step 1630, change in cost 0.0592041\n",
      "step 1640, training accuracy 0.893869\n",
      "step 1640, cost 1506.7\n",
      "step 1640, change in cost 0.05896\n",
      "step 1650, training accuracy 0.893869\n",
      "step 1650, cost 1506.64\n",
      "step 1650, change in cost 0.0582275\n",
      "step 1660, training accuracy 0.893869\n",
      "step 1660, cost 1506.58\n",
      "step 1660, change in cost 0.0574951\n",
      "step 1670, training accuracy 0.893869\n",
      "step 1670, cost 1506.53\n",
      "step 1670, change in cost 0.057373\n",
      "step 1680, training accuracy 0.893869\n",
      "step 1680, cost 1506.47\n",
      "step 1680, change in cost 0.0566406\n",
      "step 1690, training accuracy 0.893869\n",
      "step 1690, cost 1506.41\n",
      "step 1690, change in cost 0.0561523\n",
      "step 1700, training accuracy 0.893869\n",
      "step 1700, cost 1506.36\n",
      "step 1700, change in cost 0.0559082\n",
      "step 1710, training accuracy 0.893869\n",
      "step 1710, cost 1506.3\n",
      "step 1710, change in cost 0.0559082\n",
      "step 1720, training accuracy 0.893869\n",
      "step 1720, cost 1506.25\n",
      "step 1720, change in cost 0.055542\n",
      "step 1730, training accuracy 0.893869\n",
      "step 1730, cost 1506.19\n",
      "step 1730, change in cost 0.055542\n",
      "step 1740, training accuracy 0.893869\n",
      "step 1740, cost 1506.13\n",
      "step 1740, change in cost 0.0556641\n",
      "step 1750, training accuracy 0.893869\n",
      "step 1750, cost 1506.08\n",
      "step 1750, change in cost 0.0560303\n",
      "step 1760, training accuracy 0.893869\n",
      "step 1760, cost 1506.02\n",
      "step 1760, change in cost 0.0560303\n",
      "step 1770, training accuracy 0.893869\n",
      "step 1770, cost 1505.97\n",
      "step 1770, change in cost 0.0563965\n",
      "step 1780, training accuracy 0.893869\n",
      "step 1780, cost 1505.91\n",
      "step 1780, change in cost 0.0568848\n",
      "step 1790, training accuracy 0.893869\n",
      "step 1790, cost 1505.85\n",
      "step 1790, change in cost 0.057251\n",
      "step 1800, training accuracy 0.893869\n",
      "step 1800, cost 1505.79\n",
      "step 1800, change in cost 0.0577393\n",
      "step 1810, training accuracy 0.893869\n",
      "step 1810, cost 1505.74\n",
      "step 1810, change in cost 0.0583496\n",
      "step 1820, training accuracy 0.893869\n",
      "step 1820, cost 1505.68\n",
      "step 1820, change in cost 0.0584717\n",
      "step 1830, training accuracy 0.893869\n",
      "step 1830, cost 1505.62\n",
      "step 1830, change in cost 0.0595703\n",
      "step 1840, training accuracy 0.893869\n",
      "step 1840, cost 1505.56\n",
      "step 1840, change in cost 0.0596924\n",
      "step 1850, training accuracy 0.893869\n",
      "step 1850, cost 1505.5\n",
      "step 1850, change in cost 0.0600586\n",
      "step 1860, training accuracy 0.893869\n",
      "step 1860, cost 1505.44\n",
      "step 1860, change in cost 0.0601807\n",
      "step 1870, training accuracy 0.893869\n",
      "step 1870, cost 1505.38\n",
      "step 1870, change in cost 0.0601807\n",
      "step 1880, training accuracy 0.893869\n",
      "step 1880, cost 1505.32\n",
      "step 1880, change in cost 0.0601807\n",
      "step 1890, training accuracy 0.893869\n",
      "step 1890, cost 1505.26\n",
      "step 1890, change in cost 0.0601807\n",
      "step 1900, training accuracy 0.893869\n",
      "step 1900, cost 1505.2\n",
      "step 1900, change in cost 0.0604248\n",
      "step 1910, training accuracy 0.893869\n",
      "step 1910, cost 1505.14\n",
      "step 1910, change in cost 0.0595703\n",
      "step 1920, training accuracy 0.893869\n",
      "step 1920, cost 1505.08\n",
      "step 1920, change in cost 0.0594482\n",
      "step 1930, training accuracy 0.893869\n",
      "step 1930, cost 1505.02\n",
      "step 1930, change in cost 0.0596924\n",
      "step 1940, training accuracy 0.893869\n",
      "step 1940, cost 1504.96\n",
      "step 1940, change in cost 0.0592041\n",
      "step 1950, training accuracy 0.893869\n",
      "step 1950, cost 1504.9\n",
      "step 1950, change in cost 0.0599365\n",
      "step 1960, training accuracy 0.893869\n",
      "step 1960, cost 1504.84\n",
      "step 1960, change in cost 0.0600586\n",
      "step 1970, training accuracy 0.893869\n",
      "step 1970, cost 1504.78\n",
      "step 1970, change in cost 0.0604248\n",
      "step 1980, training accuracy 0.893869\n",
      "step 1980, cost 1504.72\n",
      "step 1980, change in cost 0.0610352\n",
      "step 1990, training accuracy 0.893869\n",
      "step 1990, cost 1504.66\n",
      "step 1990, change in cost 0.0616455\n",
      "final accuracy on test set: 0.904249\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAacAAAEYCAYAAAD4czk4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmY2cJyczMxs4Dg5mZnZwPn/QI3nz7BHrF4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110456f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training epochs\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_OP)\n",
    "\n",
    "    ###########################\n",
    "    ### GRAPH LIVE UPDATING ###\n",
    "    ###########################\n",
    "\n",
    "    epoch_values=[]\n",
    "    accuracy_values=[]\n",
    "    cost_values=[]\n",
    "    \n",
    "    # Turn on interactive plotting\n",
    "    plt.ion()\n",
    "    # Create the main, super plot\n",
    "    fig = plt.figure()\n",
    "    # Create two subplots on their own axes and give titles\n",
    "    ax1 = plt.subplot(\"211\")\n",
    "    ax1.set_title(\"TRAINING ACCURACY\", fontsize=10)\n",
    "    ax2 = plt.subplot(\"212\")\n",
    "    ax2.set_title(\"TRAINING COST\", fontsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    #####################\n",
    "    ### RUN THE GRAPH ###\n",
    "    #####################\n",
    "\n",
    "    ## Ops for vizualization\n",
    "    # argmax(activation_OP, 1) gives the label our model thought was most likely\n",
    "    # argmax(yGold, 1) is the correct label\n",
    "    correct_predictions_OP = tf.equal(tf.argmax(activation_OP,1),tf.argmax(yGold,1))\n",
    "    # False is 0 and True is 1, what was our average?\n",
    "    accuracy_OP = tf.reduce_mean(tf.cast(correct_predictions_OP, \"float\"))\n",
    "    # Summary op for regression output\n",
    "    activation_summary_OP = tf.summary.histogram(\"output\", activation_OP)\n",
    "    # Summary op for accuracy\n",
    "    accuracy_summary_OP = tf.summary.scalar(\"accuracy\", accuracy_OP)\n",
    "    # Summary op for cost\n",
    "    cost_summary_OP = tf.summary.scalar(\"cost\", cost_OP)\n",
    "    # Summary ops to check how variables (W, b) are updating after each iteration\n",
    "    weightSummary = tf.summary.histogram(\"weights\", weights.eval(session=sess))\n",
    "    biasSummary = tf.summary.histogram(\"biases\", bias.eval(session=sess))\n",
    "    # Merge all summaries\n",
    "    all_summary_OPS = tf.summary.merge_all()\n",
    "    # Summary writer\n",
    "    writer = tf.summary.FileWriter(\"summary_logs\", sess.graph)\n",
    "\n",
    "    # Initialize reporting variables\n",
    "    cost = 0\n",
    "    diff = 1\n",
    "\n",
    "    for i in range(numEpochs): \n",
    "        if i > 1 and diff < .0001:\n",
    "            print(\"change in cost %g; convergence.\"%diff)\n",
    "            break\n",
    "        else:\n",
    "            # Run training step\n",
    "            step = sess.run(training_OP, feed_dict={X: trainX, yGold: trainY})\n",
    "            \n",
    "            # Report occasional stats\n",
    "            if i % 10 == 0:\n",
    "                # Add epoch to epoch_values\n",
    "                epoch_values.append(i)\n",
    "                # Generate accuracy stats on test data\n",
    "                summary_results, train_accuracy, newCost = sess.run(\n",
    "                    [all_summary_OPS, accuracy_OP, cost_OP],\n",
    "                    feed_dict={X: trainX, yGold: trainY}\n",
    "                )\n",
    "                # Add accuracy to live graphing variable\n",
    "                accuracy_values.append(train_accuracy)\n",
    "                # Add cost to live graphing variable\n",
    "                cost_values.append(newCost)\n",
    "                # Write summary stats to writer\n",
    "                writer.add_summary(summary_results, i)\n",
    "                # Re-assign values for variables\n",
    "                diff = abs(newCost - cost)\n",
    "                cost = newCost\n",
    "\n",
    "                #generate print statements\n",
    "                print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "                print(\"step %d, cost %g\"%(i, newCost))\n",
    "                print(\"step %d, change in cost %g\"%(i, diff))\n",
    "\n",
    "                # Plot progress to our two subplots\n",
    "                accuracyLine, = ax1.plot(epoch_values, accuracy_values)\n",
    "                costLine, = ax2.plot(epoch_values, cost_values)\n",
    "                fig.canvas.draw()\n",
    "                time.sleep(1)\n",
    "    \n",
    "    # Create Saver\n",
    "    saver = tf.train.Saver()   \n",
    "    saver.save(sess, '/tmp/dinner-model')\n",
    "\n",
    "    # How well do we perform on held-out test data?\n",
    "    print(\"final accuracy on test set: %s\" %str(sess.run(accuracy_OP,\n",
    "                                                         feed_dict={X: testX,\n",
    "                                                                    yGold: testY})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import new set\n",
    "Now we'll bring in the 1000 most recent recipes published to Cookpad and return those which are identified by the algorithm as 'dinner' so that we can subject them to human analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape = (1000, 4060)\n"
     ]
    }
   ],
   "source": [
    "data = rec_parse.parse_newest_recipes() \n",
    "print(\"data shape =\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "cookpad.com/us/recipes/1892928\n",
      "cookpad.com/us/recipes/1858178\n",
      "cookpad.com/us/recipes/1823000\n",
      "cookpad.com/us/recipes/1808551\n",
      "cookpad.com/us/recipes/1816171\n",
      "cookpad.com/us/recipes/1802445\n",
      "cookpad.com/us/recipes/1795147\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with tf.Session() as lolo:\n",
    "    saver.restore(lolo, '/tmp/dinner-model')\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    predictions = lolo.run(activation_OP, feed_dict={X: data})\n",
    "\n",
    "    id_set = []\n",
    "    \n",
    "    for index, i in enumerate(predictions):\n",
    "        if i[0] > .8:\n",
    "            id_set.append(index)\n",
    "    \n",
    "    for n in rec_parse.convert_to_id(id_set):  \n",
    "        print('cookpad.com/us/recipes/' + n[1])\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
