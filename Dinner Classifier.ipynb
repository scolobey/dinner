{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import rec_parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Data\n",
    "To obtain data, a recipe is parsed to identify dictionary matches. Each dictionary entry corresponds to an index in recipe space. So, each recipe is represented as a vector in recipe space of dimension 4060. Beginnning with a set of 15768 recipe vectors. 10% of those vectors, at equal intervals are reserved as test data. The result should be 4 matrices, trainX representing 14191 recipe vectors, trainY representing binary labels for each of those recipes, testX representing 1577 recipe vectors on which to test, and testY reresenting labels for the test data. For the label matrices, a 1 in column 0 represents 'dinner', while a 1 in column 1 represents 'non-dinner'.\n",
    "\n",
    "A simple improvement here would be to calculate labels before splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your data is ready.\n",
      "trainX shape = (14191, 4060)\n",
      "trainY shape = (14191, 2)\n",
      "testX shape = (1577, 4060)\n",
      "testY shape = (1577, 2)\n"
     ]
    }
   ],
   "source": [
    "data = rec_parse.parse_recipes()\n",
    "trainX, testX = rec_parse.split(data)\n",
    "\n",
    "trainY = np.zeros((len(data)-1577, 2), dtype=np.int)\n",
    "testY = np.zeros([1577, 2], dtype=np.int)\n",
    "\n",
    "for ind, row in enumerate(trainY):\n",
    "    if ind > 1500:\n",
    "        trainY[ind][1] = 1\n",
    "    else:\n",
    "        trainY[ind][0] = 1\n",
    "\n",
    "for ind, row in enumerate(testY):\n",
    "    if ind > 150:\n",
    "        testY[ind][1] = 1\n",
    "    else:\n",
    "        testY[ind][0] = 1\n",
    "        \n",
    "print(\"Your data is ready.\")\n",
    "print(\"trainX shape =\", trainX.shape)\n",
    "print(\"trainY shape =\", trainY.shape)\n",
    "print(\"testX shape =\", testX.shape)\n",
    "print(\"testY shape =\", testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize graph features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numFeatures = trainX.shape[1]\n",
    "numLabels = trainY.shape[1]\n",
    "\n",
    "numEpochs = 500\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learningRate = tf.train.exponential_decay(learning_rate=0.0008,\n",
    "                                          global_step= 1,\n",
    "                                          decay_steps=trainX.shape[0],\n",
    "                                          decay_rate= 0.95,\n",
    "                                          staircase=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, numFeatures])\n",
    "yGold = tf.placeholder(tf.float32, [None, numLabels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Variable/read:0\", shape=(4060, 2), dtype=float32)\n",
      "Tensor(\"Variable_1/read:0\", shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "weights = tf.Variable(tf.random_normal([numFeatures,numLabels],\n",
    "                                       mean=0,\n",
    "                                       stddev=(np.sqrt(6/numFeatures+\n",
    "                                                         numLabels+1)),\n",
    "                                       name=\"weights\"))\n",
    "\n",
    "bias = tf.Variable(tf.random_normal([1,numLabels],\n",
    "                                    mean=0,\n",
    "                                    stddev=(np.sqrt(6/numFeatures+numLabels+1)),\n",
    "                                    name=\"bias\"))\n",
    "\n",
    "init_OP = tf.global_variables_initializer()\n",
    "\n",
    "for i in tf.global_variables():\n",
    "    print(i)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The algorithm\n",
    "Multiply trainX(14191, 4060) by weights(4060, 2), resulting in apply_weights_OP(14191, 2).\n",
    "And add the bias(1, 2), resulting in add_bias_OP(14191, 2)\n",
    "\n",
    "Activate using rectified linear:\n",
    "f(x) = max(0, x)\n",
    "\n",
    "Calculate loss using Mean Squared Error:\n",
    "$\\frac{1}{n}\\sum_{i=0}^n ({y}_{estimated}-{y}_{real})^2$\n",
    "\n",
    "Optimize using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FEEDFORWARD ALGORITHM\n",
    "apply_weights_OP = tf.matmul(X, weights, name=\"apply_weights\")\n",
    "add_bias_OP = tf.add(apply_weights_OP, bias, name=\"add_bias\")\n",
    "activation_OP = tf.nn.sigmoid(add_bias_OP, name=\"activation\")\n",
    "\n",
    "# COST FUNCTION i.e. MEAN SQUARED ERROR\n",
    "cost_OP = tf.nn.l2_loss(activation_OP-yGold, name=\"squared_error_cost\")\n",
    "\n",
    "# OPTIMIZATION ALGORITHM i.e. GRADIENT DESCENT\n",
    "training_OP = tf.train.GradientDescentOptimizer(learningRate).minimize(cost_OP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.687337\n",
      "step 0, cost 5002.46\n",
      "step 0, change in cost 5002.46\n",
      "step 10, training accuracy 0.880558\n",
      "step 10, cost 1980.17\n",
      "step 10, change in cost 3022.29\n",
      "step 20, training accuracy 0.887887\n",
      "step 20, cost 1778.98\n",
      "step 20, change in cost 201.188\n",
      "step 30, training accuracy 0.889578\n",
      "step 30, cost 1707.82\n",
      "step 30, change in cost 71.1562\n",
      "step 40, training accuracy 0.891199\n",
      "step 40, cost 1660.72\n",
      "step 40, change in cost 47.1041\n",
      "step 50, training accuracy 0.89141\n",
      "step 50, cost 1628.3\n",
      "step 50, change in cost 32.4224\n",
      "step 60, training accuracy 0.891974\n",
      "step 60, cost 1609.25\n",
      "step 60, change in cost 19.0468\n",
      "step 70, training accuracy 0.892185\n",
      "step 70, cost 1595.61\n",
      "step 70, change in cost 13.6392\n",
      "step 80, training accuracy 0.892397\n",
      "step 80, cost 1584.95\n",
      "step 80, change in cost 10.663\n",
      "step 90, training accuracy 0.892608\n",
      "step 90, cost 1576.52\n",
      "step 90, change in cost 8.43079\n",
      "step 100, training accuracy 0.892678\n",
      "step 100, cost 1570.17\n",
      "step 100, change in cost 6.35193\n",
      "step 110, training accuracy 0.89289\n",
      "step 110, cost 1565.21\n",
      "step 110, change in cost 4.95374\n",
      "step 120, training accuracy 0.89289\n",
      "step 120, cost 1561.02\n",
      "step 120, change in cost 4.19202\n",
      "step 130, training accuracy 0.893031\n",
      "step 130, cost 1557.3\n",
      "step 130, change in cost 3.71606\n",
      "step 140, training accuracy 0.893101\n",
      "step 140, cost 1553.93\n",
      "step 140, change in cost 3.37427\n",
      "step 150, training accuracy 0.893101\n",
      "step 150, cost 1550.89\n",
      "step 150, change in cost 3.03662\n",
      "step 160, training accuracy 0.893242\n",
      "step 160, cost 1548.24\n",
      "step 160, change in cost 2.65637\n",
      "step 170, training accuracy 0.893313\n",
      "step 170, cost 1545.95\n",
      "step 170, change in cost 2.28918\n",
      "step 180, training accuracy 0.893313\n",
      "step 180, cost 1543.96\n",
      "step 180, change in cost 1.98389\n",
      "step 190, training accuracy 0.893383\n",
      "step 190, cost 1542.23\n",
      "step 190, change in cost 1.73816\n",
      "step 200, training accuracy 0.893383\n",
      "step 200, cost 1540.67\n",
      "step 200, change in cost 1.55786\n",
      "step 210, training accuracy 0.893454\n",
      "step 210, cost 1539.23\n",
      "step 210, change in cost 1.44287\n",
      "step 220, training accuracy 0.893524\n",
      "step 220, cost 1537.86\n",
      "step 220, change in cost 1.36438\n",
      "step 230, training accuracy 0.893524\n",
      "step 230, cost 1536.57\n",
      "step 230, change in cost 1.29431\n",
      "step 240, training accuracy 0.893524\n",
      "step 240, cost 1535.33\n",
      "step 240, change in cost 1.23535\n",
      "step 250, training accuracy 0.893595\n",
      "step 250, cost 1534.14\n",
      "step 250, change in cost 1.19568\n",
      "step 260, training accuracy 0.893665\n",
      "step 260, cost 1532.96\n",
      "step 260, change in cost 1.17896\n",
      "step 270, training accuracy 0.893665\n",
      "step 270, cost 1531.77\n",
      "step 270, change in cost 1.18909\n",
      "step 280, training accuracy 0.893665\n",
      "step 280, cost 1530.55\n",
      "step 280, change in cost 1.22095\n",
      "step 290, training accuracy 0.893665\n",
      "step 290, cost 1529.28\n",
      "step 290, change in cost 1.26233\n",
      "step 300, training accuracy 0.893735\n",
      "step 300, cost 1528.01\n",
      "step 300, change in cost 1.27161\n",
      "step 310, training accuracy 0.893735\n",
      "step 310, cost 1526.8\n",
      "step 310, change in cost 1.21094\n",
      "step 320, training accuracy 0.893735\n",
      "step 320, cost 1525.69\n",
      "step 320, change in cost 1.1084\n",
      "step 330, training accuracy 0.893735\n",
      "step 330, cost 1524.69\n",
      "step 330, change in cost 0.999756\n",
      "step 340, training accuracy 0.893806\n",
      "step 340, cost 1523.78\n",
      "step 340, change in cost 0.909546\n",
      "step 350, training accuracy 0.893806\n",
      "step 350, cost 1522.93\n",
      "step 350, change in cost 0.849854\n",
      "step 360, training accuracy 0.893806\n",
      "step 360, cost 1522.12\n",
      "step 360, change in cost 0.816284\n",
      "step 370, training accuracy 0.893806\n",
      "step 370, cost 1521.32\n",
      "step 370, change in cost 0.792847\n",
      "step 380, training accuracy 0.893806\n",
      "step 380, cost 1520.57\n",
      "step 380, change in cost 0.759033\n",
      "step 390, training accuracy 0.893806\n",
      "step 390, cost 1519.86\n",
      "step 390, change in cost 0.705933\n",
      "step 400, training accuracy 0.893876\n",
      "step 400, cost 1519.21\n",
      "step 400, change in cost 0.653198\n",
      "step 410, training accuracy 0.893876\n",
      "step 410, cost 1518.6\n",
      "step 410, change in cost 0.607544\n",
      "step 420, training accuracy 0.893876\n",
      "step 420, cost 1518.03\n",
      "step 420, change in cost 0.565308\n",
      "step 430, training accuracy 0.893876\n",
      "step 430, cost 1517.51\n",
      "step 430, change in cost 0.527344\n",
      "step 440, training accuracy 0.893876\n",
      "step 440, cost 1517\n",
      "step 440, change in cost 0.502686\n",
      "step 450, training accuracy 0.893876\n",
      "step 450, cost 1516.51\n",
      "step 450, change in cost 0.494629\n",
      "step 460, training accuracy 0.893876\n",
      "step 460, cost 1516\n",
      "step 460, change in cost 0.512451\n",
      "step 470, training accuracy 0.893947\n",
      "step 470, cost 1515.44\n",
      "step 470, change in cost 0.557861\n",
      "step 480, training accuracy 0.893947\n",
      "step 480, cost 1514.87\n",
      "step 480, change in cost 0.565918\n",
      "step 490, training accuracy 0.893947\n",
      "step 490, cost 1514.4\n",
      "step 490, change in cost 0.476196\n",
      "final accuracy on test set: 0.904249\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAacAAAEYCAYAAAD4czk4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110c5bf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training epochs\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_OP)\n",
    "\n",
    "    ###########################\n",
    "    ### GRAPH LIVE UPDATING ###\n",
    "    ###########################\n",
    "\n",
    "    epoch_values=[]\n",
    "    accuracy_values=[]\n",
    "    cost_values=[]\n",
    "    \n",
    "    # Turn on interactive plotting\n",
    "    plt.ion()\n",
    "    # Create the main, super plot\n",
    "    fig = plt.figure()\n",
    "    # Create two subplots on their own axes and give titles\n",
    "    ax1 = plt.subplot(\"211\")\n",
    "    ax1.set_title(\"TRAINING ACCURACY\", fontsize=10)\n",
    "    ax2 = plt.subplot(\"212\")\n",
    "    ax2.set_title(\"TRAINING COST\", fontsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    #####################\n",
    "    ### RUN THE GRAPH ###\n",
    "    #####################\n",
    "\n",
    "    ## Ops for vizualization\n",
    "    # argmax(activation_OP, 1) gives the label our model thought was most likely\n",
    "    # argmax(yGold, 1) is the correct label\n",
    "    correct_predictions_OP = tf.equal(tf.argmax(activation_OP,1),tf.argmax(yGold,1))\n",
    "    # False is 0 and True is 1, what was our average?\n",
    "    accuracy_OP = tf.reduce_mean(tf.cast(correct_predictions_OP, \"float\"))\n",
    "    # Summary op for regression output\n",
    "    activation_summary_OP = tf.summary.histogram(\"output\", activation_OP)\n",
    "    # Summary op for accuracy\n",
    "    accuracy_summary_OP = tf.summary.scalar(\"accuracy\", accuracy_OP)\n",
    "    # Summary op for cost\n",
    "    cost_summary_OP = tf.summary.scalar(\"cost\", cost_OP)\n",
    "    # Summary ops to check how variables (W, b) are updating after each iteration\n",
    "    weightSummary = tf.summary.histogram(\"weights\", weights.eval(session=sess))\n",
    "    biasSummary = tf.summary.histogram(\"biases\", bias.eval(session=sess))\n",
    "    # Merge all summaries\n",
    "    all_summary_OPS = tf.summary.merge_all()\n",
    "    # Summary writer\n",
    "    writer = tf.summary.FileWriter(\"summary_logs\", sess.graph)\n",
    "\n",
    "    # Initialize reporting variables\n",
    "    cost = 0\n",
    "    diff = 1\n",
    "\n",
    "    for i in range(numEpochs): \n",
    "        if i > 1 and diff < .0001:\n",
    "            print(\"change in cost %g; convergence.\"%diff)\n",
    "            break\n",
    "        else:\n",
    "            # Run training step\n",
    "            step = sess.run(training_OP, feed_dict={X: trainX, yGold: trainY})\n",
    "            \n",
    "            # Report occasional stats\n",
    "            if i % 10 == 0:\n",
    "                # Add epoch to epoch_values\n",
    "                epoch_values.append(i)\n",
    "                # Generate accuracy stats on test data\n",
    "                summary_results, train_accuracy, newCost = sess.run(\n",
    "                    [all_summary_OPS, accuracy_OP, cost_OP],\n",
    "                    feed_dict={X: trainX, yGold: trainY}\n",
    "                )\n",
    "                # Add accuracy to live graphing variable\n",
    "                accuracy_values.append(train_accuracy)\n",
    "                # Add cost to live graphing variable\n",
    "                cost_values.append(newCost)\n",
    "                # Write summary stats to writer\n",
    "                writer.add_summary(summary_results, i)\n",
    "                # Re-assign values for variables\n",
    "                diff = abs(newCost - cost)\n",
    "                cost = newCost\n",
    "\n",
    "                #generate print statements\n",
    "                print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "                print(\"step %d, cost %g\"%(i, newCost))\n",
    "                print(\"step %d, change in cost %g\"%(i, diff))\n",
    "\n",
    "                # Plot progress to our two subplots\n",
    "                accuracyLine, = ax1.plot(epoch_values, accuracy_values)\n",
    "                costLine, = ax2.plot(epoch_values, cost_values)\n",
    "                fig.canvas.draw()\n",
    "                time.sleep(1)\n",
    "    \n",
    "    # Create Saver\n",
    "    saver = tf.train.Saver()   \n",
    "    saver.save(sess, '/tmp/dinner-model')\n",
    "\n",
    "    # How well do we perform on held-out test data?\n",
    "    print(\"final accuracy on test set: %s\" %str(sess.run(accuracy_OP,\n",
    "                                                         feed_dict={X: testX,\n",
    "                                                                    yGold: testY})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import new set\n",
    "Now we'll bring in the 100 most recent recipes published to Cookpad and return those which are identified by the algorithm as 'dinner'so that we can subject them to human analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape = (1001, 4060)\n"
     ]
    }
   ],
   "source": [
    "data = rec_parse.parse_newest_recipes() \n",
    "print(\"data shape =\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "[  1.04353713e-11   1.00000000e+00]\n",
      "72 0.999886\n",
      "76 0.800014\n",
      "146 0.999475\n",
      "314 0.998206\n",
      "554 0.956514\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as lolo:\n",
    "    saver.restore(lolo, '/tmp/dinner-model')\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    predictions = lolo.run(activation_OP, feed_dict={X: data})\n",
    "    print(predictions[0])\n",
    "    for index, i in enumerate(predictions):\n",
    "        if i[0] > .8:\n",
    "            print(index, i[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
