{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import rec_parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Data\n",
    "To obtain data, a recipe is parsed to identify dictionary matches. Each dictionary entry corresponds to an index in recipe space. So, each recipe is represented as a vector in recipe space of dimension 4060. Beginnning with a set of 15768 recipe vectors. 10% of those vectors, at equal intervals are reserved as test data. The result should be 4 matrices, trainX representing 14191 recipe vectors, trainY representing binary labels for each of those recipes, testX representing 1577 recipe vectors on which to test, and testY reresenting labels for the test data. For the label matrices, a 1 in column 0 represents 'dinner', while a 1 in column 1 represents 'non-dinner'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your data is ready.\n",
      "trainX shape = (6998, 3530)\n",
      "trainY shape = (6998, 2)\n",
      "testX shape = (778, 3530)\n",
      "testY shape = (778, 2)\n",
      "7776\n"
     ]
    }
   ],
   "source": [
    "rec_parse.clear_data()\n",
    "\n",
    "dinner_count = rec_parse.initialize_csv(\"rec_dinner_tag.csv\")\n",
    "breakfast_count = rec_parse.initialize_csv(\"rec_breakfast_tag.csv\")\n",
    "dessert_count = rec_parse.initialize_csv(\"rec_modified_dessert_tag.csv\")\n",
    "\n",
    "data = rec_parse.generate_training_data()\n",
    "\n",
    "dinner_labels = np.zeros((dinner_count, 2), dtype=np.int)\n",
    "breakfast_labels = np.zeros((breakfast_count, 2), dtype=np.int)\n",
    "dessert_labels = np.zeros((dessert_count, 2), dtype=np.int)\n",
    "                          \n",
    "for label in dinner_labels:\n",
    "    label[1] = 1\n",
    "                          \n",
    "for label in breakfast_labels:\n",
    "    label[1] = 1\n",
    "                          \n",
    "for label in dessert_labels:\n",
    "    label[0] = 1\n",
    "    \n",
    "labels = np.concatenate((dinner_labels, breakfast_labels), axis=0)\n",
    "labels = np.concatenate((labels, dessert_labels), axis=0)\n",
    "\n",
    "trainX, testX = rec_parse.split(data)\n",
    "trainY, testY = rec_parse.split(labels)\n",
    "                          \n",
    "# # data = rec_parse.parse_recipes()\n",
    "                          \n",
    "# # trainX, testX = rec_parse.split(data)\n",
    "\n",
    "# # trainY = np.zeros((len(data)-1577, 2), dtype=np.int)\n",
    "# # testY = np.zeros([1577, 2], dtype=np.int)\n",
    "\n",
    "# # for ind, row in enumerate(trainY):\n",
    "# #     if ind > 1500:\n",
    "# #         trainY[ind][1] = 1\n",
    "# #     else:\n",
    "# #         trainY[ind][0] = 1\n",
    "\n",
    "# # for ind, row in enumerate(testY):\n",
    "# #     if ind > 150:\n",
    "# #         testY[ind][1] = 1\n",
    "# #     else:\n",
    "# #         testY[ind][0] = 1\n",
    "        \n",
    "print(\"Your data is ready.\")\n",
    "print(\"trainX shape =\", trainX.shape)\n",
    "print(\"trainY shape =\", trainY.shape)\n",
    "print(\"testX shape =\", testX.shape)\n",
    "print(\"testY shape =\", testY.shape)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize graph features\n",
    "\n",
    "At line 4, numEpochs is set very low at 100. This low number of epochs results in a model that builds quickly, but has low accuracy. to experiment with a more accurate model, increase numEpochs to at least 2000. To train a production model, you would likely run close to 30k, and probably continue running until cost no longer changes, or does so minimally from epoch to epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numFeatures = trainX.shape[1]\n",
    "numLabels = trainY.shape[1]\n",
    "\n",
    "numEpochs = 27000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learningRate = tf.train.exponential_decay(learning_rate=0.0008,\n",
    "                                          global_step= 1,\n",
    "                                          decay_steps=trainX.shape[0],\n",
    "                                          decay_rate= 0.95,\n",
    "                                          staircase=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, numFeatures])\n",
    "yGold = tf.placeholder(tf.float32, [None, numLabels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Variable/read:0\", shape=(3530, 2), dtype=float32)\n",
      "Tensor(\"Variable_1/read:0\", shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "weights = tf.Variable(tf.random_normal([numFeatures,numLabels],\n",
    "                                       mean=0,\n",
    "                                       stddev=(np.sqrt(6/numFeatures+\n",
    "                                                         numLabels+1)),\n",
    "                                       name=\"weights\"))\n",
    "\n",
    "bias = tf.Variable(tf.random_normal([1,numLabels],\n",
    "                                    mean=0,\n",
    "                                    stddev=(np.sqrt(6/numFeatures+numLabels+1)),\n",
    "                                    name=\"bias\"))\n",
    "\n",
    "init_OP = tf.global_variables_initializer()\n",
    "\n",
    "for i in tf.global_variables():\n",
    "    print(i)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The algorithm\n",
    "Multiply trainX(14191, 4060) by weights(4060, 2), resulting in apply_weights_OP(14191, 2).\n",
    "And add the bias(1, 2), resulting in add_bias_OP(14191, 2)\n",
    "\n",
    "Activate using rectified linear:\n",
    "f(x) = max(0, x)\n",
    "\n",
    "Calculate loss using Mean Squared Error:\n",
    "$\\frac{1}{n}\\sum_{i=0}^n ({y}_{estimated}-{y}_{real})^2$\n",
    "\n",
    "Optimize using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FEEDFORWARD ALGORITHM\n",
    "apply_weights_OP = tf.matmul(X, weights, name=\"apply_weights\")\n",
    "add_bias_OP = tf.add(apply_weights_OP, bias, name=\"add_bias\")\n",
    "activation_OP = tf.nn.sigmoid(add_bias_OP, name=\"activation\")\n",
    "\n",
    "# COST FUNCTION i.e. MEAN SQUARED ERROR\n",
    "cost_OP = tf.nn.l2_loss(activation_OP-yGold, name=\"squared_error_cost\")\n",
    "\n",
    "# OPTIMIZATION ALGORITHM i.e. GRADIENT DESCENT\n",
    "training_OP = tf.train.GradientDescentOptimizer(learningRate).minimize(cost_OP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.65533\n",
      "step 0, cost 2447.09\n",
      "step 0, change in cost 2447.09\n",
      "step 10, training accuracy 0.891969\n",
      "step 10, cost 934.209\n",
      "step 10, change in cost 1512.88\n",
      "step 20, training accuracy 0.923407\n",
      "step 20, cost 688.472\n",
      "step 20, change in cost 245.737\n",
      "step 30, training accuracy 0.934124\n",
      "step 30, cost 585.94\n",
      "step 30, change in cost 102.532\n",
      "step 40, training accuracy 0.942412\n",
      "step 40, cost 524.958\n",
      "step 40, change in cost 60.9822\n",
      "step 50, training accuracy 0.947414\n",
      "step 50, cost 482.158\n",
      "step 50, change in cost 42.7997\n",
      "step 60, training accuracy 0.950271\n",
      "step 60, cost 449.294\n",
      "step 60, change in cost 32.8645\n",
      "step 70, training accuracy 0.952701\n",
      "step 70, cost 422.483\n",
      "step 70, change in cost 26.8104\n",
      "step 80, training accuracy 0.95513\n",
      "step 80, cost 399.932\n",
      "step 80, change in cost 22.5509\n",
      "step 90, training accuracy 0.956559\n",
      "step 90, cost 381.315\n",
      "step 90, change in cost 18.6179\n",
      "step 100, training accuracy 0.957131\n",
      "step 100, cost 367.076\n",
      "step 100, change in cost 14.2386\n",
      "step 110, training accuracy 0.957845\n",
      "step 110, cost 356.227\n",
      "step 110, change in cost 10.8491\n",
      "step 120, training accuracy 0.958274\n",
      "step 120, cost 347.253\n",
      "step 120, change in cost 8.97375\n",
      "step 130, training accuracy 0.958845\n",
      "step 130, cost 339.546\n",
      "step 130, change in cost 7.70743\n",
      "step 140, training accuracy 0.959417\n",
      "step 140, cost 333.009\n",
      "step 140, change in cost 6.53647\n"
     ]
    }
   ],
   "source": [
    "# Training epochs\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_OP)\n",
    "\n",
    "    ###########################\n",
    "    ### GRAPH LIVE UPDATING ###\n",
    "    ###########################\n",
    "\n",
    "    epoch_values=[]\n",
    "    accuracy_values=[]\n",
    "    cost_values=[]\n",
    "    \n",
    "    # Turn on interactive plotting\n",
    "    plt.ion()\n",
    "    # Create the main, super plot\n",
    "    fig = plt.figure()\n",
    "    # Create two subplots on their own axes and give titles\n",
    "    ax1 = plt.subplot(\"211\")\n",
    "    ax1.set_title(\"TRAINING ACCURACY\", fontsize=10)\n",
    "    ax2 = plt.subplot(\"212\")\n",
    "    ax2.set_title(\"TRAINING COST\", fontsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    #####################\n",
    "    ### RUN THE GRAPH ###\n",
    "    #####################\n",
    "\n",
    "    ## Ops for vizualization\n",
    "    # argmax(activation_OP, 1) gives the label our model thought was most likely\n",
    "    # argmax(yGold, 1) is the correct label\n",
    "    correct_predictions_OP = tf.equal(tf.argmax(activation_OP,1),tf.argmax(yGold,1))\n",
    "    # False is 0 and True is 1, what was our average?\n",
    "    accuracy_OP = tf.reduce_mean(tf.cast(correct_predictions_OP, \"float\"))\n",
    "    # Summary op for regression output\n",
    "    activation_summary_OP = tf.summary.histogram(\"output\", activation_OP)\n",
    "    # Summary op for accuracy\n",
    "    accuracy_summary_OP = tf.summary.scalar(\"accuracy\", accuracy_OP)\n",
    "    # Summary op for cost\n",
    "    cost_summary_OP = tf.summary.scalar(\"cost\", cost_OP)\n",
    "    # Summary ops to check how variables (W, b) are updating after each iteration\n",
    "    weightSummary = tf.summary.histogram(\"weights\", weights.eval(session=sess))\n",
    "    biasSummary = tf.summary.histogram(\"biases\", bias.eval(session=sess))\n",
    "    # Merge all summaries\n",
    "    all_summary_OPS = tf.summary.merge_all()\n",
    "    # Summary writer\n",
    "    writer = tf.summary.FileWriter(\"summary_logs\", sess.graph)\n",
    "\n",
    "    # Initialize reporting variables\n",
    "    cost = 0\n",
    "    diff = 1\n",
    "\n",
    "    for i in range(numEpochs): \n",
    "        if i > 1 and diff < .0001:\n",
    "            print(\"change in cost %g; convergence.\"%diff)\n",
    "            break\n",
    "        else:\n",
    "            # Run training step\n",
    "            step = sess.run(training_OP, feed_dict={X: trainX, yGold: trainY})\n",
    "            \n",
    "            # Report occasional stats\n",
    "            if i % 10 == 0:\n",
    "                # Add epoch to epoch_values\n",
    "                epoch_values.append(i)\n",
    "                # Generate accuracy stats on test data\n",
    "                summary_results, train_accuracy, newCost = sess.run(\n",
    "                    [all_summary_OPS, accuracy_OP, cost_OP],\n",
    "                    feed_dict={X: trainX, yGold: trainY}\n",
    "                )\n",
    "                # Add accuracy to live graphing variable\n",
    "                accuracy_values.append(train_accuracy)\n",
    "                # Add cost to live graphing variable\n",
    "                cost_values.append(newCost)\n",
    "                # Write summary stats to writer\n",
    "                writer.add_summary(summary_results, i)\n",
    "                # Re-assign values for variables\n",
    "                diff = abs(newCost - cost)\n",
    "                cost = newCost\n",
    "\n",
    "                #generate print statements\n",
    "                print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "                print(\"step %d, cost %g\"%(i, newCost))\n",
    "                print(\"step %d, change in cost %g\"%(i, diff))\n",
    "\n",
    "                # Plot progress to our two subplots\n",
    "                accuracyLine, = ax1.plot(epoch_values, accuracy_values)\n",
    "                costLine, = ax2.plot(epoch_values, cost_values)\n",
    "                fig.canvas.draw()\n",
    "                time.sleep(1)\n",
    "    \n",
    "    # Create Saver\n",
    "    saver = tf.train.Saver()   \n",
    "    saver.save(sess, '/tmp/dinner-model')\n",
    "\n",
    "    # How well do we perform on held-out test data?\n",
    "    print(\"final accuracy on test set: %s\" %str(sess.run(accuracy_OP,\n",
    "                                                         feed_dict={X: testX,\n",
    "                                                                    yGold: testY})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import new set\n",
    "Now we'll bring in the 1000 most recent recipes published to Cookpad and return those which are identified by the algorithm as 'dinner' so that we can subject them to human analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = rec_parse.parse_newest_recipes() \n",
    "print(\"data shape =\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with tf.Session() as lolo:\n",
    "    saver.restore(lolo, '/tmp/dinner-model')\n",
    "    print(\"Model restored.\")\n",
    "    print(\"Checking \", len(data), \"recipes.\")\n",
    "    \n",
    "    predictions = lolo.run(activation_OP, feed_dict={X: data})\n",
    "\n",
    "    \n",
    "    id_set = []\n",
    "    \n",
    "    for index, i in enumerate(predictions):\n",
    "        if i[0] > .9:\n",
    "            id_set.append(index)\n",
    "    \n",
    "    for n in rec_parse.convert_to_id(id_set):  \n",
    "        print('cookpad.com/us/recipes/' + n[1])\n",
    "        \n",
    "    print(len(id_set), \"dinner recipes detected\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
